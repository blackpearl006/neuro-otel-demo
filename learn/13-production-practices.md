# Lesson 13: Production Practices

**Estimated time**: 45-55 minutes

---

## ğŸ¯ Learning Objectives

By the end of this lesson, you will:

âœ… Know how to run observability in production
âœ… Understand cost optimization strategies
âœ… Learn retention and storage policies
âœ… Know security best practices
âœ… Understand high availability and scaling

---

## Running in Production

Taking observability from development to production requires careful planning.

---

## Cost Optimization

**Problem**: Observability can be expensive at scale.

**Key costs**:
- Storage (traces, metrics, logs)
- Compute (collector, backends)
- Network (data transfer)

---

### Strategy 1: Sampling

**Reduce trace volume** without losing critical data.

#### Head-Based Sampling

Keep a percentage of all traces:

```python
from opentelemetry.sdk.trace.sampling import TraceIdRatioBased

# Keep 10% of traces
sampler = TraceIdRatioBased(0.1)
```

**Pros**: Simple, predictable costs
**Cons**: May lose important traces (errors, slow requests)

---

#### Tail-Based Sampling (Recommended)

Keep all errors and slow traces, sample the rest:

**Collector config**:

```yaml
processors:
  tail_sampling:
    decision_wait: 10s
    policies:
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      - name: slow
        type: latency
        latency:
          threshold_ms: 2000  # > 2 seconds
      - name: sample_rest
        type: probabilistic
        probabilistic:
          sampling_percentage: 5  # Only 5% of normal traces
```

**Result**:
- 100% of errors kept
- 100% of slow requests kept
- 5% of fast, successful requests kept
- **~80-90% cost reduction**

---

### Strategy 2: Reduce Metric Cardinality

**Problem**: High cardinality = many time series = expensive.

**Bad**:

```python
# âŒ Creates 100,000 time series (one per user)
requests_counter.add(1, {"user_id": user_id})
```

**Good**:

```python
# âœ… Creates 3 time series (one per tier)
requests_counter.add(1, {"user_tier": "free|pro|enterprise"})
```

**Best practice**:
- Keep label cardinality < 100 per label
- Use aggregations instead of per-entity metrics
- Drop unnecessary labels

---

### Strategy 3: Retention Policies

**Don't keep data forever** - it's expensive.

#### Prometheus Retention

```yaml
# docker-compose.yml
prometheus:
  command:
    - '--storage.tsdb.retention.time=30d'  # Keep 30 days
    - '--storage.tsdb.retention.size=50GB'  # Or 50GB max
```

**Recommendation**: 15-30 days for metrics

---

#### Loki Retention

```yaml
# loki-config.yaml
compactor:
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150

limits_config:
  retention_period: 168h  # 7 days
```

**Recommendation**: 7-14 days for logs

---

#### Tempo Retention

```yaml
# tempo-config.yaml
compactor:
  compaction:
    block_retention: 168h  # 7 days
```

**Recommendation**: 7-14 days for traces

---

### Strategy 4: Compress and Optimize

**Enable compression** everywhere:

```yaml
# Collector
exporters:
  otlp:
    compression: gzip  # Reduces network traffic by ~70%
```

**Use object storage**:
- S3, GCS, Azure Blob (cheap long-term storage)
- Lifecycle policies (move old data to cheaper tiers)

---

### Strategy 5: Filter Unnecessary Data

**Drop health checks and debug endpoints**:

```yaml
processors:
  filter:
    traces:
      span:
        - 'attributes["http.target"] =~ "/health|/ping|/debug"'
```

**Result**: 20-30% reduction in trace volume for typical web apps.

---

## Security Best Practices

### 1. Don't Log Sensitive Data

**Bad**:

```python
logger.info(f"User logged in: email={email}, password={password}")  # âŒ
```

**Good**:

```python
logger.info(f"User logged in: user_id={user_id}")  # âœ…
```

**Always scrub**:
- Passwords
- API keys
- Credit card numbers
- SSNs
- PII (personal identifiable information)

---

### 2. Redact in the Collector

**Add processor** to remove sensitive attributes:

```yaml
processors:
  attributes:
    actions:
      - key: password
        action: delete
      - key: credit_card
        action: delete
      - key: ssn
        action: delete
      - key: email
        action: hash  # Hash instead of plaintext
```

---

### 3. Enable TLS

**Encrypt data in transit**:

```yaml
exporters:
  otlp:
    endpoint: tempo:4317
    tls:
      insecure: false  # Enable TLS
      cert_file: /certs/client.crt
      key_file: /certs/client.key
```

---

### 4. Implement Access Control

**Grafana**:
- Enable authentication
- Use RBAC (Role-Based Access Control)
- Limit dashboard editing to admins
- Audit logs for access

**Backends**:
- Enable authentication on Prometheus, Loki, Tempo
- Use API keys or OAuth
- Network policies (firewall rules)

---

### 5. Data Retention and Privacy

**Comply with regulations** (GDPR, HIPAA):
- Delete data after retention period
- Provide user data deletion on request
- Anonymize user identifiers
- Document what data is collected

---

## High Availability and Scaling

### Collector Scaling

**Problem**: Single collector is a bottleneck and single point of failure.

**Solution**: Deploy multiple collectors.

#### Horizontal Scaling (Multiple Instances)

```
App 1 â”€â”€â”
App 2 â”€â”€â”¼â”€â”€â†’ Load Balancer â”€â”€â”¬â”€â”€â†’ Collector 1 â”€â”€â”
App 3 â”€â”€â”˜                     â”œâ”€â”€â†’ Collector 2 â”€â”€â”¼â”€â”€â†’ Backends
                              â””â”€â”€â†’ Collector 3 â”€â”€â”˜
```

**Benefits**:
- High availability (one dies, others continue)
- Load distribution
- Horizontal scalability

**Implementation**:
- Use Kubernetes with multiple collector pods
- Use load balancer (round-robin or least connections)
- Stateless collectors (no local state)

---

#### Agent + Gateway Architecture

```
App 1 â†’ Agent 1 â”€â”€â”
App 2 â†’ Agent 2 â”€â”€â”¼â”€â”€â†’ Gateway Collectors â”€â”€â†’ Backends
App 3 â†’ Agent 3 â”€â”€â”˜
```

**Agent (on each host)**:
- Local buffering
- Initial processing (filtering, sampling)
- Resilience to gateway failures

**Gateway (centralized)**:
- Advanced processing
- Routing to multiple backends
- Cost optimization (sampling, filtering)

---

### Backend Scaling

#### Prometheus

**Single instance** is often enough for small-medium deployments.

**For larger scale**:
- **Thanos**: Multi-cluster, long-term storage
- **Cortex**: Horizontally scalable Prometheus
- **Mimir**: Grafana's scalable metrics backend

---

#### Loki

**Modes**:
- **Monolithic**: Single binary (small scale)
- **Microservices**: Separate components (large scale)
  - Distributor (receives logs)
  - Ingester (writes to storage)
  - Querier (handles queries)
  - Compactor (compacts blocks)

**Scaling**:
- Add more ingesters for write throughput
- Add more queriers for read throughput
- Use object storage (S3, GCS)

---

#### Tempo

**Modes**:
- **Monolithic**: Single binary (small scale)
- **Microservices**: Separate components (large scale)

**Scaling**:
- Horizontal scaling of each component
- Object storage for traces
- Memcached for caching

---

## Alerting Strategies

### What to Alert On

**Do alert on**:
- High error rate (> 5%)
- High latency (P95 > SLA)
- Service down (up metric = 0)
- Disk/memory near capacity (> 85%)
- Certificate expiration (< 7 days)

**Don't alert on**:
- Individual errors (too noisy)
- Small latency blips (< 1 second)
- Metrics that auto-recover
- Development/test environments

---

### Alert Severity Levels

**Critical (P0)**:
- Service completely down
- Data loss occurring
- Security breach

**Action**: Page on-call engineer immediately

---

**High (P1)**:
- Degraded service (> 10% error rate)
- SLA breach imminent

**Action**: Page during business hours, alert off-hours

---

**Medium (P2)**:
- Elevated errors (5-10%)
- Performance degradation

**Action**: Ticket created, investigate next business day

---

**Low (P3)**:
- Warning signs (increasing errors)
- Resource utilization high

**Action**: Monitor, investigate when convenient

---

### Alert Fatigue Prevention

**Problem**: Too many alerts â†’ ignored alerts â†’ real issues missed.

**Solutions**:

1. **Reduce noise**: Only alert on actionable issues
2. **Group alerts**: Combine related alerts
3. **Silence during maintenance**: Temporary mutes
4. **Tune thresholds**: Adjust based on false positives
5. **Escalation policies**: Route based on severity

---

## Monitoring the Monitoring

**Don't forget to monitor your observability stack!**

### Collector Health

**Metrics to watch**:
```promql
# Collector receiving data?
otelcol_receiver_accepted_spans

# Collector exporting data?
otelcol_exporter_sent_spans

# Collector errors?
otelcol_exporter_send_failed_spans

# Collector memory usage?
process_resident_memory_bytes{job="otel-collector"}
```

**Alerts**:
- Collector not receiving data (input problem)
- Collector not exporting data (output problem)
- High error rate on exports (backend issues)
- High memory usage (resource pressure)

---

### Backend Health

**Prometheus**:
```promql
# Prometheus up?
up{job="prometheus"}

# Scrape success rate?
up{job="otel-collector"}

# Query duration?
prometheus_engine_query_duration_seconds
```

**Loki**:
```promql
# Loki up?
up{job="loki"}

# Ingester accepting logs?
loki_distributor_bytes_received_total
```

**Tempo**:
```promql
# Tempo up?
up{job="tempo"}

# Traces ingested?
tempo_ingester_traces_created_total
```

---

## Capacity Planning

### Estimating Storage Needs

**Metrics (Prometheus)**:
- **Formula**: `samples/sec * retention_days * bytes_per_sample`
- **Example**: 100k samples/sec * 30 days * 2 bytes = 518GB

**Logs (Loki)**:
- **Formula**: `logs/sec * avg_log_size * retention_days`
- **Example**: 1000 logs/sec * 500 bytes * 7 days = 302GB

**Traces (Tempo)**:
- **Formula**: `traces/sec * avg_trace_size * retention_days * (1 - sampling_rate)`
- **Example**: 100 traces/sec * 50KB * 7 days * 0.1 = 302GB

---

### Estimating Compute Needs

**Collector**:
- CPU: 1 core per 10k spans/sec
- Memory: 2GB baseline + 1GB per 10k spans/sec

**Prometheus**:
- CPU: 1 core per 1M samples/sec
- Memory: 2GB per 1M active time series

**Loki**:
- CPU: 1 core per 100MB/sec ingest
- Memory: 4GB baseline + scale with query load

**Tempo**:
- CPU: 1 core per 500 traces/sec
- Memory: 4GB baseline + scale with query load

---

## Disaster Recovery

### Backup Strategy

**What to back up**:
- âœ… Grafana dashboards (export JSON)
- âœ… Alert rules (export YAML)
- âœ… Collector configs (version control)
- âœ… Datasource configs (version control)

**What NOT to back up**:
- âŒ Metrics (ephemeral, recreated from apps)
- âŒ Logs (ephemeral, recreated from apps)
- âŒ Traces (ephemeral, recreated from apps)

**Exception**: If observability data is business-critical (audit logs), back up to cold storage.

---

### Recovery Procedures

**Collector failure**:
1. Deploy new collector instance
2. Point apps to new collector
3. Data loss: Only data during outage

**Prometheus failure**:
1. Deploy new Prometheus
2. Data loss: Only data during outage
3. Historical data: Gone (use remote write to backup)

**Grafana failure**:
1. Deploy new Grafana
2. Import backed-up dashboards
3. Reconnect datasources

---

## Cost Monitoring

### Track Observability Costs

**Set up billing alerts**:
- Storage costs (GB stored)
- Compute costs (CPU/memory hours)
- Network costs (data transfer)

**Optimize based on trends**:
- If storage growing: Reduce retention or sampling
- If compute growing: Optimize queries or scale backends
- If network growing: Enable compression

---

### Cost Per Service

**Tag resources** by service:

```yaml
resource:
  attributes:
    - key: service.name
      value: neuro-preprocess
    - key: cost.center
      value: research
```

**Query costs** by service:
```promql
sum by (service_name) (storage_bytes)
```

**Chargeback**: Bill teams based on their observability usage.

---

## Best Practices Summary

### Configuration Management

âœ… Store configs in version control (Git)
âœ… Use infrastructure as code (Terraform, Ansible)
âœ… Document all changes
âœ… Test configs in staging before production

---

### Performance

âœ… Enable compression everywhere
âœ… Use batching (collector processor)
âœ… Implement sampling for high-traffic services
âœ… Monitor collector/backend resource usage

---

### Reliability

âœ… Deploy multiple collector instances
âœ… Use load balancers
âœ… Set up alerting on observability stack health
âœ… Have disaster recovery procedures

---

### Security

âœ… Scrub sensitive data (passwords, PII)
âœ… Enable TLS for data in transit
âœ… Implement access control
âœ… Regular security audits

---

### Cost

âœ… Implement sampling (tail-based)
âœ… Set retention policies (7-30 days)
âœ… Reduce metric cardinality
âœ… Filter unnecessary data (health checks)
âœ… Monitor and optimize costs regularly

---

## Real-World Production Setup

Here's a typical production architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Applications (VMs/Pods)               â”‚
â”‚  App 1, App 2, App 3, ... App N                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ OTLP
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Agent Collectors (on each host/pod)         â”‚
â”‚  â€¢ Local buffering                                 â”‚
â”‚  â€¢ Basic filtering                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ OTLP (compressed)
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gateway Collectors (centralized, HA)             â”‚
â”‚  â€¢ Load balanced (3+ instances)                    â”‚
â”‚  â€¢ Tail-based sampling                             â”‚
â”‚  â€¢ Advanced filtering                              â”‚
â”‚  â€¢ Routing to backends                             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚
       â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tempo    â”‚  â”‚Prometheusâ”‚  â”‚  Loki    â”‚
â”‚ (Scaled) â”‚  â”‚ (HA pair)â”‚  â”‚ (Scaled) â”‚
â”‚          â”‚  â”‚          â”‚  â”‚          â”‚
â”‚ S3       â”‚  â”‚ S3       â”‚  â”‚ S3       â”‚
â”‚ Storage  â”‚  â”‚ Remote   â”‚  â”‚ Storage  â”‚
â”‚          â”‚  â”‚ Write    â”‚  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚             â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚  Grafana  â”‚
              â”‚   (HA)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key features**:
- Agent collectors for local buffering
- Gateway collectors for centralized processing (HA)
- Object storage (S3) for cheap long-term storage
- Grafana HA for reliability
- Load balancers at every tier

---

## Key Takeaways

1. **Cost optimization**: Sampling, retention policies, cardinality reduction
2. **Security**: Scrub sensitive data, enable TLS, access control
3. **High availability**: Multiple collectors, load balancing, agent+gateway
4. **Alerting**: Alert on actionable issues, prevent alert fatigue
5. **Capacity planning**: Estimate storage and compute needs
6. **Monitor the monitoring**: Watch collector and backend health
7. **Disaster recovery**: Back up configs, not data
8. **Best practices**: Version control, IaC, testing, documentation

---

## Quiz

1. **What's the difference between head-based and tail-based sampling?**
   <details>
   <summary>Click to see answer</summary>
   Head-based: Decide at trace start (simple, may lose errors). Tail-based: Decide after trace completes (keeps errors/slow, more complex).
   </details>

2. **What should you set for retention policies?**
   <details>
   <summary>Click to see answer</summary>
   Metrics: 15-30 days, Logs: 7-14 days, Traces: 7-14 days. Balance between debuggability and cost.
   </details>

3. **What data should you back up in your observability stack?**
   <details>
   <summary>Click to see answer</summary>
   Dashboards, alert rules, configs, datasources. NOT metrics/logs/traces (ephemeral, recreated from apps).
   </details>

4. **How do you prevent alert fatigue?**
   <details>
   <summary>Click to see answer</summary>
   Only alert on actionable issues, group related alerts, tune thresholds, silence during maintenance, use escalation policies.
   </details>

---

## Next Steps

ğŸ‰ **Congratulations!** You now know how to run observability in production!

**Next**: [Lesson 14: Advanced Topics â†’](14-advanced-topics.md)

In the final lesson, you'll learn:
- Advanced sampling strategies
- Cardinality explosion and solutions
- Custom exporters and processors
- OpenTelemetry SDK internals

---

**Progress**: âœ… Lessons 1-13 complete | â¬œ 1 lesson remaining
